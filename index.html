
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="img/icon.png">
  <title>Pratul Srinivasan</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Pratul Srinivasan</name>
        </p>
        <p>
          I'm a research scientist at <a href="https://research.google/">Google Research</a>. I recently completed my PhD in the <a href="https://eecs.berkeley.edu/">EECS Department</a> at UC Berkeley, advised by <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a> and <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>, where I was a part of the <a href="https://bair.berkeley.edu/">Berkeley AI Research (BAIR)</a> lab. My main research interests lie at the intersection of computer vision, computer graphics, and machine learning.
        </p>
        <p>
          During my PhD, I interned twice at <a href="https://research.google/">Google Research</a>: at Mountain View in 2017 (hosted by <a href="https://jonbarron.info/">Jon Barron</a> in <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy's</a> group) and at New York City in 2018 (hosted by <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>).
        </p>
        <p>
          I graduated from Duke University in 2014, where I majored in Biomedical Engineering and Computer Science. At Duke, I worked with <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a> on research problems in medical computer vision.
        </p>
        <p>
          I grew up in Palo Alto, CA and graduated from Henry M. Gunn High School in 2010.
        </p>
        <p align=center>
          <a href="mailto:pratul@berkeley.edu">Email</a> &nbsp/&nbsp
          <a href="pdf/cv.pdf">CV</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=aYyDsZ0AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
          <a href="https://twitter.com/_pratul_">Twitter</a>
        </p>
        </td>
        <td width="33%">
	  <a href="img/avatar.jpg">
        <img src="img/avatar_circle.png" width="250px"></a>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <p>
            * denotes equal contribution co-authorship

          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nerv_image'><img src='img/hotdog.gif' width="160"></div>
                <img src='img/hotdog.png' width="160">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://pratulsrinivasan.github.io/nerv/">
                <papertitle>NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              </br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>arXiv</em>, 2020
              <br>
              <a href="https://pratulsrinivasan.github.io/nerv/">project page</a> /
              <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
              <a href="https://arxiv.org/abs/2012.03927">arXiv</a> /
              <a href="bib/nerv2020.bib">bibtex</a>
              <p></p>
              <p>We recover relightable NeRF-like models using neural approximations of expensive visibility integrals, so we can simulate complex volumetric light transport during training.</p>
            </td>
          </tr>

          <tr onmouseout="winr_stop()" onmouseover="winr_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/notre_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/notre.jpg' width="160">
              </div>
              <script type="text/javascript">
                function winr_start() {
                  document.getElementById('winr_image').style.opacity = "1";
                }

                function winr_stop() {
                  document.getElementById('winr_image').style.opacity = "0";
                }
                winr_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://www.matthewtancik.com/learnit">
                <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://www.linkedin.com/in/terrance-wang/">Terrance Wang</a>,
              <a href="https://www.linkedin.com/in/divi-schmidt-262044180/">Divi Schmidt</a>,
              <strong>Pratul Srinivasan</strong>,
              </br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>arXiv</em>, 2020
              <br>
              <a href="http://www.matthewtancik.com/learnit">project page</a> /
              <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
              <a href="https://arxiv.org/abs/2012.02189">arXiv</a> /
              <a href="bib/learnit2020.bib">bibtex</a>
              <p></p>
              <p>We use meta-learning to find weight initializations for coordinate-based MLPs that allow them to converge faster and generalize better.</p>
            </td>
          </tr>

          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ff_image'><img src='img/lion_ff.jpg' width="160"></div>
                <img src='img/lion_none.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://bmild.github.io//fourfeat/index.html">
                <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>,
              <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
              <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>NeurIPS</em>, 2020 <font color="tomato"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io//fourfeat/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
              <a href="https://github.com/tancik/fourier-feature-networks">code</a> /
              <a href="bib/fourfeat2020.bib">bibtex</a>
              <p></p>
              <p>Mapping input coordinates with simple Fourier features before passing them to a fully-connected network enables the network to learn much higher-frequency functions.</p>
            </td>
          </tr>

          <tr onmouseout="nrf_stop()" onmouseover="nrf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nrf_image'><video  width="160" muted autoplay loop>
                <source src="img/neural_reflectance.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/neural_reflectance.png' width="160">
              </div>
              <script type="text/javascript">
                function nrf_start() {
                  document.getElementById('nrf_image').style.opacity = "1";
                }

                function nrf_stop() {
                  document.getElementById('nrf_image').style.opacity = "0";
                }
                nrf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2008.03824">
                <papertitle>Neural Reflectance Fields for Appearance Acquisition</papertitle>
              </a>
              <br>
              <a href="http://cseweb.ucsd.edu/~bisai/">Sai Bi*</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu*</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
              <a href="http://www.miloshasan.net/">Milos Hasan</a>,
              <a href="http://yannickhold.com/">Yannick Hold-Geoffroy</a>,
              <a href="https://cseweb.ucsd.edu/~kriegman/">David Kriegman</a>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
        <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.03824">arXiv</a> /
              <a href="bib/nrf2020.bib">bibtex</a>
              <p></p>
              <p>We recover relightable NeRF-like models by predicting per-location BRDFs and surface normals, and marching light rays through the NeRV volume to compute visibility.</p>
            </td>
          </tr>

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nerf_image'><img src='img/vase_small.gif' width="160"></div>
                <img src='img/vase_still.png' width="160">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://www.matthewtancik.com/nerf">
                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2020 <font color="tomato"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/nerf">project page</a> /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">video</a> /
              <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo">technical overview</a> /
              <a href="https://github.com/bmild/nerf">code</a> /
	            <a href="https://www.youtube.com/watch?v=nCpGStnayHk">two minute papers</a> /
              <a href="bib/nerf2020.bib">bibtex</a>
              <p></p>
              <p>We optimize a simple neural network to represent a scene as a 5D function (3D volume + 2D view direction) from just a set of images, and synthesize photorealistic novel views.</p>
            </td>
          </tr>

          <tr onmouseout="mdp_stop()" onmouseover="mdp_start()">
            <td width="25%">
              <div class="two" id='mdp_image'><video  width="160" muted autoplay loop>
              <source src="img/mdp.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
                <img src='img/mdp.png' width="160">
              </div>
              <script type="text/javascript">
                function mdp_start() {
                  document.getElementById('mdp_image').style.opacity = "1";
                }

                function mdp_stop() {
                  document.getElementById('mdp_image').style.opacity = "0";
                }
                mdp_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2008.01815">
                <papertitle>Deep Multi Depth Panoramas for View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/kaienlin2576/">Kai-En Lin</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>,
              <a href="https://pratulsrinivasan.github.io/">Ben Mildenhall</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="http://yannickhold.com/">Yannick Hold-Geoffroy</a>,
            </br>
              <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>,
              <a href="https://qisun.me/">Qi Sun</a>,
              <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.01815">arXiv</a> /
              <a href="https://cseweb.ucsd.edu/~zex014/papers/2020_mdp/2020_mdp.mp4">video</a> /
              <a href="bib/mdp2020.bib">bibtex</a>
              <p></p>
              <p>We represent scenes as multi-layer panoramas with depth for VR view synthesis.</p>
            </td>
          </tr>

          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='lh_image'><img src='img/rings_crop.gif' width="160"></div>
                <img src='img/rings.png' width="160">
              </div>
              <script type="text/javascript">
                function lighthouse_start() {
                  document.getElementById('lh_image').style.opacity = "1";
                }

                function lighthouse_stop() {
                  document.getElementById('lh_image').style.opacity = "0";
                }
                lighthouse_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://pratulsrinivasan.github.io/lighthouse/">
                <papertitle>Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a href="https://pratulsrinivasan.github.io/lighthouse/">project page</a> /
              <a href="https://arxiv.org/abs/2003.08367">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=KsiZpUFPqIU">video</a> /
              <a href="https://github.com/pratulsrinivasan/lighthouse">code</a> /
              <a href="bib/lighthouse2020.bib">bibtex</a>
              <p></p>
              <p>We predict a multiscale light volume from an input stereo pair, and render this volume to compute illumination at any 3D point for relighting inserted virtual objects.</p>
            </td>
          </tr>

          <tr onmouseout="llff_stop()" onmouseover="llff_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='llff_image'><img src='img/fern160.gif' width="160"></div>
                <img src='img/fern.jpg' width="160">
              </div>
              <script type="text/javascript">
                function llff_start() {
                  document.getElementById('llff_image').style.opacity = "1";
                }

                function llff_stop() {
                  document.getElementById('llff_image').style.opacity = "0";
                }
                llff_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://bmild.github.io/llff/">
                <papertitle>Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://scholar.google.com/citations?user=yZMAlU4AAAAJ">Rodrigo Ortiz-Cayon</a>,
              <a href="http://faculty.cs.tamu.edu/nimak/">Nima Khademi Kalantari</a>,
            </br>
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://abhishekkar.info/">Abhishek Kar</a>
              <br>
        <em>SIGGRAPH</em>, 2019
              <br>
              <a href="https://bmild.github.io/llff/">project page</a> /
              <a href="https://arxiv.org/abs/1905.00889">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=LY6MgDUzS3M">video</a> /
              <a href="https://github.com/Fyusion/LLFF">code</a> /
              <a href="bib/llff2019.bib">bibtex</a>
              <p></p>
              <p>We develop a deep learning method for rendering novel views of complex real world scenes from a small number of images, and analyze it with light field sampling theory.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='mpi_image'><img src='img/mpi_after.jpg' width="160"></div>
                <img src='img/mpi_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mpi_start() {
                  document.getElementById('mpi_image').style.opacity = "1";
                }

                function mpi_stop() {
                  document.getElementById('mpi_image').style.opacity = "0";
                }
                mpi_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1905.00413">
                <papertitle>Pushing the Boundaries of View Extrapolation with Multiplane Images</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>, <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019 &nbsp <font color="tomato"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1905.00413">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /
              <a href="https://github.com/google-research/google-research/tree/master/mpi_extrapolation">code</a> /
              <a href="bib/SrinivasanCVPR2019.bib">bibtex</a>
              <p></p>
              <p>We use Fourier theory to show the limits of view extrapolation with multiplane images, and develop a deep learning pipeline with 3D inpainting for better view extrapolation results.</p>
            </td>
          </tr>

          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='aperture_image'><img src='img/aperture_after.jpg'></div>
                <img src='img/aperture_before.jpg'>
              </div>
              <script type="text/javascript">
                function aperture_start() {
                  document.getElementById('aperture_image').style.opacity = "1";
                }

                function aperture_stop() {
                  document.getElementById('aperture_image').style.opacity = "0";
                }
                aperture_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1711.07933">
                <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1711.07933">arXiv</a> /
              <a href="https://github.com/google/aperture_supervision">code</a> /
              <a href="bib/Srinivasan2018.bib">bibtex</a>
              <p></p>
              <p>We train a neural network to estimate a depth map from a single image using only images with different-sized apertures as supervision, and use this to synthesize artificial bokeh.</p>
            </td>
          </tr>

          <tr onmouseout="cb_stop()" onmouseover="cb_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cb_image'><img src='img/chromablur_after.jpg' width="160"></div>
                <img src='img/chromablur_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function cb_start() {
                  document.getElementById('cb_image').style.opacity = "1";
                }

                function cb_stop() {
                  document.getElementById('cb_image').style.opacity = "0";
                }
                cb_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://bankslab.berkeley.edu/publications/chromablur/">
                <papertitle>ChromaBlur: Rendering Chromatic Eye Aberration Improves Accommodation and Realism</papertitle>
              </a>
              <br>
              <a href="https://steven.cholewiak.com/">Steven A. Cholewiak</a>,
              <a href="https://www.dur.ac.uk/physics/staff/profiles/?id=246">Gordon D. Love</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://bankslab.berkeley.edu/">Martin S. Banks</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2017 &nbsp <font color="tomato"></font>
              <br>
              <a href="http://bankslab.berkeley.edu/publications/chromablur/">project page</a> /
              <a href="https://www.youtube.com/watch?v=oGZgEmkmvvg&feature=youtu.be">video</a> /
              <a href="bib/chromablur2017.bib">bibtex</a>
              <p></p>
              <p>We show that properly considering the eye's aberrations when rendering for VR displays increases perceived realism and helps drive accomodation.</p>
            </td>
          </tr>

          <tr onmouseout="lf_stop()" onmouseover="lf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='lf_image'><img src='img/lfsyn_after.gif' width="160"></div>
                <img src='img/lfsyn_before.gif' width="160">
              </div>
              <script type="text/javascript">
                function lf_start() {
                  document.getElementById('lf_image').style.opacity = "1";
                }

                function lf_stop() {
                  document.getElementById('lf_image').style.opacity = "0";
                }
                lf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1708.03292">
                <papertitle>Learning to Synthesize a 4D RGBD Light Field from a Single Image</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>, <a href="https://ssnl.github.io/">Tongzhou Wang</a>,
              Ashwin Sreelal,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2017 &nbsp <font color="tomato"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1708.03292">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=yLCvWoQLnms&feature=youtu.be">video</a> /
              <a href="https://github.com/pratulsrinivasan/Local_Light_Field_Synthesis">code</a> /
              <a href="pdf/ICCV17_LF_Synthesis_Supplementary.pdf">supplementary PDF</a> /
              <a href="bib/lfsyn2017.bib">bibtex</a>
              <p></p>
              <p>We train a neural network to predict ray depths and RGB colors for a local light field around a single input image.</p>
            </td>
          </tr>

          <tr onmouseout="deblur_stop()" onmouseover="deblur_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='deblur_image'><img src='img/deblur_ours.gif' width="160"></div>
                <img src='img/deblur_blur.gif' width="160">
              </div>
              <script type="text/javascript">
                function deblur_start() {
                  document.getElementById('deblur_image').style.opacity = "1";
                }

                function deblur_stop() {
                  document.getElementById('deblur_image').style.opacity = "0";
                }
                deblur_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1704.05416">
                <papertitle>Light Field Blind Motion Deblurring</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>Conference Computer Vision and Pattern Recognition (CVPR)</em>, 2017 &nbsp <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1704.05416">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=rtukre-ErmI&feature=youtu.be">video</a> /
              <a href="https://github.com/pratulsrinivasan/Light_Field_Blind_Motion_Deblurring">code</a> /
              <a href="https://pratulsrinivasan.github.io/deblur_html/supplementary.html">additional results</a> /
              <a href="bib/lfdeblur2017.bib">bibtex</a>
              <p></p>
              <p>We develop Fourier theory to describe the effects of camera motion on light fields, and an optimization algorithm for deblurring light fields captured with unknown camera motion.</p>
            </td>
          </tr>

          <tr onmouseout="sf_stop()" onmouseover="sf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='sf_image'><img src='img/ow_after.gif' width="160"></div>
                <img src='img/ow_before.gif' width="160">
              </div>
              <script type="text/javascript">
                function sf_start() {
                  document.getElementById('sf_image').style.opacity = "1";
                }

                function sf_stop() {
                  document.getElementById('sf_image').style.opacity = "0";
                }
                sf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="pdf/ICCV15_LF_ORIENTED_WINDOWS.pdf">
                <papertitle>Oriented Light-Field Windows for Scene Flow</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=P_GSjQMAAAAJ&hl=en">Michael W. Tao</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2015
              <br>
              <a href="pdf/ICCV15_LF_ORIENTED_WINDOWS.pdf">paper PDF</a> /
              <a href="code/ICCV15_LF_SCENE_FLOW.zip">code</a> /
              <a href="https://youtu.be/hENxM4lBVXo">video</a> /
              <a href="bib/lfsceneflow2015.bib">bibtex</a>
              <p></p>
              <p>We develop a 4D light field descriptor and an algorithm to use these to compute scene flow (3D motion of observed points) from two captured light fields.</p>
            </td>
          </tr>

          <tr onmouseout="lfd_stop()" onmouseover="lfd_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='lfd_image'><img src='img/lfd_after.png' width="160"></div>
                <img src='img/lfd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function lfd_start() {
                  document.getElementById('lfd_image').style.opacity = "1";
                }

                function lfd_stop() {
                  document.getElementById('lfd_image').style.opacity = "0";
                }
                lfd_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tao_Depth_From_Shading_2015_CVPR_paper.pdf">
                <papertitle>Shape Estimation from Shading, Defocus, and Correspondence Using Light-Field Angular Coherence</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=P_GSjQMAAAAJ&hl=en">Michael W. Tao</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=4g-njrYAAAAJ&hl=en">Sunil Hadap</a>,
              <a href="https://www.cs.princeton.edu/~smr/">Szymon Rusinkiewicz</a>,
            </br>
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>IEEE Transactions on Pattern Matching and Machine Intelligence (PAMI)</em>, 2017 and <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015
              <br>
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tao_Depth_From_Shading_2015_CVPR_paper.pdf">conference PDF</a> /
              <a href="https://cseweb.ucsd.edu/~ravir/normals_PAMI.pdf">journal PDF</a> /
              <a href="code/CVPR15_LF_DEPTH_SHADING.zip">code</a> /
              <a href="bib/lfdepth2015.bib">bibtex</a>
              <p></p>
              <p>We develop an algorithm that jointly considers cues from defocus, correspondence, and shading to estimate better depths from a light field.</p>
            </td>
          </tr>

          <tr onmouseout="dd_stop()" onmouseover="dd_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='dd_image'><img src='img/dd_after.png' width="160"></div>
                <img src='img/dd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function dd_start() {
                  document.getElementById('dd_image').style.opacity = "1";
                }

                function dd_stop() {
                  document.getElementById('dd_image').style.opacity = "0";
                }
                dd_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-10-3568">
                <papertitle>Fully Automated Detection of Diabetic Macular Edema and Dry Age-Related Macular Degeneration from Optical Coherence Tomography Images</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              Leo A. Kim, Priyatham S. Mettu, Scott W. Cousins, Grant M. Comer,
              <a href="https://bme.duke.edu/faculty/joseph-izatt">Joseph A. Izatt</a>,
              <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a>
              <br>
              <em>Biomedical Optics Express</em>, 2014
              <br>
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-10-3568">journal article</a> /
              <a href="http://people.duke.edu/~sf59/Srinivasan_BOE_2014_dataset.htm">dataset</a> /
              <a href="bib/octclassify2014.bib">bibtex</a>
              <p></p>
              <p>We develop a classification algorithm to detect diseases from OCT images of the retina.</p>
            </td>
          </tr>

          <tr onmouseout="seg_stop()" onmouseover="seg_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='seg_image'><img src='img/seg_after.png'></div>
                <img src='img/seg_before.png'>
              </div>
              <script type="text/javascript">
                function seg_start() {
                  document.getElementById('seg_image').style.opacity = "1";
                }

                function seg_stop() {
                  document.getElementById('seg_image').style.opacity = "0";
                }
                seg_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1704.05416">
                <papertitle>Automatic Segmentation of up to Ten Layer Boundaries in SD-OCT Images of the Mouse Retina With and Without Missing Layers due to Pathology</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              Stephanie J. Heflin,
              <a href="https://bme.duke.edu/faculty/joseph-izatt">Joseph A. Izatt</a>,
              <a href="https://dukeeyecenter.duke.edu/about/faculty/vadim-y-arshavsky-phd">Vadim Y. Arshavsky</a>,
              <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a>
              <br>
              <em>Biomedical Optics Express</em>, 2014
              <br>
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-2-348">journal article</a> /
              <a href="bib/octseg2014.bib">bibtex</a>
              <p></p>
              <p>We develop a segmentation algorithm to quantify the shape of retinal layers in OCT images that is robust to deformations due to disease.</p>
            </td>
          </tr>

      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Teaching</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr onmouseout="box_stop()" onmouseover="box_start()">
        <td width="25%">
          <div class="one">
            <div class="two" id='box_image'><img src='img/cbox.gif' width="160"></div>
            <img src='img/cbox_before.gif' width="160">
          </div>
          <script type="text/javascript">
            function box_start() {
              document.getElementById('box_image').style.opacity = "1";
            }

            function box_stop() {
              document.getElementById('box_image').style.opacity = "0";
            }
            box_stop()
          </script>
        </td>
        <td valign="center" width="75%">
          <p>
            <a href="https://cs184.eecs.berkeley.edu/sp18">
            <papertitle>CS184 - Computer Graphics and Imaging, Spring 2018 (GSI)</papertitle>
            </a>
            <br><br>
            <a href="https://cs184.eecs.berkeley.edu/sp19">
            <papertitle>CS184 - Computer Graphics and Imaging, Spring 2019 (GSI)</papertitle>
            </a>
            <br>
          </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>. <br>
          Last updated May 2020.
          <!-- http://www.cs.princeton.edu/~namana/ http://people.csail.mit.edu/janner/ http://www.sjoerdvansteenkiste.com/-->
	    </font>
        </p>
        </td>
      </tr>
      </table>

    </td>
    </tr>
  </table>
  </body>
</html>
